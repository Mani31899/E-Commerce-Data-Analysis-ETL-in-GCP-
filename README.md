# E-Commerce Catalog Search & BigQuery Analytics - GCP ETL Pipeline

## Project Overview
This project revolves around building a comprehensive data pipeline for analyzing Amazon books catalog data using Google Cloud Platform (GCP) services. It integrates Apache Beam, PySpark, Google Data Proc, and BigQuery for efficient data processing, analytics, and reporting.

## Key Components
**Google Cloud Storage:** Utilized for storing the Amazon books catalog data.

**Google Data Proc:** Employed to create and manage clusters for executing PySpark jobs.

**Apache Beam:** Used for building data processing pipelines to transfer data from Cloud Storage to BigQuery.

**BigQuery:** Leveraged for performing data analytics and generating reports using SQL queries.

**Command Line Interface (CLI):** Employed for job submission and cluster management.

## Project Workflow
**Setup on Google Cloud Platform:** Creation of a Storage Bucket using the CLI and configuration of a Data Proc cluster with specified parameters.

**Data Processing with PySpark:** Execution of PySpark files on Data Proc clusters to process and query the Amazon books catalog data.

**Google Data Flow Pipeline:** Construction of a seamless pipeline using Apache Beam to transfer data from the Storage Bucket to BigQuery, facilitating efficient data movement and transformation.

**BigQuery Analytics and Reporting:** Utilization of BigQuery's scalable and distributed architecture for conducting data analytics and generating insightful reports using SQL queries.

## Skill
SQL: Proficiency in writing SQL queries for data analysis and reporting purposes.

GCP: Google Cloud Services like DataFlow, Apache Beam, Dataproc and BigQeury
